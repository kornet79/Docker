# Docker 총 내용

# Container란?

> [Container](https://cloud.google.com/containers/?hl=ko)란 어플리케이션이 동작하기 위해서 필요한 요소(실행 파일, 어플리케이션 엔진 등)을 패키지화하고 격리하는 기술
> 

## Container를 왜 사용하는걸까?

- 일관성 있는 환경
    - 개발자는 컨테이너를 이용해, 다른 애플리케이션과 분리된 예측 가능한 환경을 생성할 수 있다. 컨테이너는 애플리케이션에 필요한 소프트웨어 종속 항목(프로그래밍 언어 런타임 및 기타 소프트웨어 라이브러리의 특정 버전 등)도 포함할 수 있다. 개발자의 관점에서 이 모든 요소는 애플리케이션이 배포되는 최종 위치에 관계없이 항상 일관성이 있기 때문에, 그 결과 자연히 생산성이 향상될 수밖에 없다. 개발자와 IT 운영팀이 버그를 잡고 환경 차이를 진단하던 시간을 줄이고 사용자에게 신규 기능을 제공하는 데 집중할 수 있기 때문이다. 또한 개발자가 개발 및 테스트 환경에서 세운 가정이 프로덕션 환경에서 그대로 실현될 것이기 때문에 버그 수 자체도 감소한다.
- 폭넓은 구동 환경
    - 컨테이너는 Linux, Windows, Mac 운영체제, 가상 머신, 베어메탈, 개발자의 컴퓨터, 데이터 센터, 온프레미스 환경, 퍼블릭 클라우드 등 사실상 어느 환경에서나 구동되므로 개발 및 배포가 크게 쉬워진다. 컨테이너용 [Docker 이미지 형식](https://cloud.google.com/container-registry/docs/image-formats?hl=ko)은 워낙 널리 사용되기 때문에 **이식성**도 매우 뛰어난다. 소프트웨어 구동 환경이 무엇이든 컨테이너를 사용할 수 있다.
- 격리
    - 컨테이너는 CPU, 메모리, 저장소, 네트워크 리소스를 운영체제 수준에서 가상화하여 개발자에게 기타 애플리케이션으로부터 논리적으로 격리된 운영체제 샌드박스 환경을 제공한다.
    - 따라서 간소화된 배포 및 릴리스 프로세스를 통해 신속하고 자주 배포할 수 있고, 이는 리소스 활용성이 증가함을 의미한다.
    

## Container의 작동 원리

컨테이너는 cgroup과 namespace와 같은 [리눅스 커널](https://www.kernel.org/doc/Documentation/) 기반의 기술을 이용해서 프로세스를 완벽하게 격리하여 분리된 환경을 만들고 실행한다.

**cGroup**은 각 프로세스 혹은 컨테이너가 소비할 수 있는 호스트OS의 리소스를 공유하고 제한할 수 있게 한다. 이것은 호스트의 하드웨어 리소스의 서비스 거부 공격(denial of sevice attacks)을 방지하므로 리소스 활용과 보안 모두에서 중요하다. Container는 미리 정의된 제약 조건하에서 CPU와 메모리를 공유할 수 있다.

**namespace**는 운영체제 내에서 프로세스가 가지는 다른 프로세스, 네트워킹, 파일시스템, 사용자 ID 구성 요소 등의 '가시성'을 제한하여 프로세스 상호작용을 따로 구분하는 방식이다. namespace를 통해 컨테이너 프로세스는 동일한 namespace에 있는 것들만 볼 수 있도록 제한된다. 다른 컨테이너에서 만들어진 프로세스나 호스트 프로세스는 컨테이너 프로세스에서 직접 접근할 수 없다.

### [cGroup](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/ch01)?

> Control Group의 약자로, 시스템의 CPU 시간, 시스템 메모리, 네트워크 대역폭과 같은 자원을 제한하고 격리할 수 있는 커널 기능
> 

**cgroups**(control groups의 약자)는 [프로세스](https://ko.wikipedia.org/wiki/%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4)들의 [자원의 사용](https://ko.wikipedia.org/w/index.php?title=%EC%8B%9C%EC%8A%A4%ED%85%9C_%EC%9E%90%EC%9B%90&action=edit&redlink=1)(CPU, 메모리, 디스크 입출력, 네트워크 등)을 제한하고 격리시키는 [리눅스 커널](https://ko.wikipedia.org/wiki/%EB%A6%AC%EB%88%85%EC%8A%A4_%EC%BB%A4%EB%84%90) 기능이다.

[구글](https://ko.wikipedia.org/wiki/%EA%B5%AC%EA%B8%80)의 엔지니어들이 2006년에 이 기능에 대한 작업에 착수하였고 당시 이름은 "프로세스 컨테이너"(process container)였다.[[1]](https://ko.wikipedia.org/wiki/Cgroups#cite_note-1) 2007년 말에 리눅스 커널 문맥에서 "컨테이너"라는 용어의 의미가 여러 개이므로 혼란을 방지하기 위해 이름이 "컨트롤 그룹"(control groups)으로 변경되었으며, 컨트롤 그룹 기능은 2008년 1월에 출시된 커널 버전 2.6.24에 [리눅스 커널 메인라인](https://ko.wikipedia.org/wiki/%EB%A6%AC%EB%88%85%EC%8A%A4_%EC%BB%A4%EB%84%90)으로 병합되었다.[[2]](https://ko.wikipedia.org/wiki/Cgroups#cite_note-lwn-notes-2) 그 뒤로 개발자들은 수많은 새로운 기능과 컨트롤러들을 추가해오고 있는데, 이를테면 [kernfs](https://ko.wikipedia.org/wiki/Kernfs) 지원,[[3]](https://ko.wikipedia.org/wiki/Cgroups#cite_note-3) [방화벽](https://ko.wikipedia.org/wiki/%EB%B0%A9%ED%99%94%EB%B2%BD),[[4]](https://ko.wikipedia.org/wiki/Cgroups#cite_note-4) 통합된 계층구조를 포함한다.[[5]](https://ko.wikipedia.org/wiki/Cgroups#cite_note-5)

cGroup으로 컨테이너 안의 프로세스에 대해 자원을 제한함으로써 특정 컨테이너가 호스트 OS의 자원을 모두 점유하여 사용하는 일을 막는다.

- cGroup의 서브시스템
    - [blkio](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/ch-subsystems_and_tunable_parameters#sec-blkio) - 물리 드라이브 (예: 디스크, 솔리드 스테이트, USB 등)와 같은 블록 장치(Block Device)에 대한 입력/출력 액세스에 제한을 설정한다.
    - [cpu](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu) - CPU에 cgroup 작업 액세스를 제공하기 위해 스케줄러를 사용한다.
    - [cpuacct](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpuacct) - cgroup의 작업에 사용된 CPU 자원에 대한 보고서를 자동으로 생성한다.
    - [cpuset](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpuset) - 개별 CPU (멀티코어 시스템에서) 및 메모리 노드를 cgroup의 작업에 할당한다.
    - [devices](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-devices) - cgroup의 작업 단위로 장치에 대한 액세스를 허용하거나 거부한다.
    - [freezer](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-freezer) - cgroup의 작업을 일시 중지하거나 다시 시작한다.
    - [memory](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-memory) - cgroup의 작업에서 사용되는 메모리에 대한 제한을 설정하고 이러한 작업에서 사용되는 메모리 자원에 대한 보고서를 자동으로 생성한다.
    - [net_cls](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/sec-net_cls) - Linux 트래픽 컨트롤러 (tc)가 특정 cgroup 작업에서 발생하는 패킷을 식별하게 하는 클래식 식별자 (classid)를 사용하여 네트워크 패킷에 태그를 지정한다.
    - [net_prio](https://www.kernel.org/doc/Documentation/cgroup-v1/net_prio.txt) - cgroup의 작업에서 생성되는 네트워크 트래픽의 우선순위를 지정한다.

### [Namespace](https://en.wikipedia.org/wiki/Linux_namespaces)?

> 하나의 시스템에서 프로세스를 격리(Isolation)시킬 수 있는 가상화 기술이다.
> 

Namespace는 커널 인스턴스를 만들지 않고 기존의 리소스들을 필요한 만큼의 Namespace로 분리하여 묶어 관리하는 방법으로 사용한다. 커널이 부팅된 후 관리 자원은 각각의 초기 Default Namespace에서 관리한다. 그 후 사용자의 필요에 따라 Namespace를 추가하여 자원들을 별도로 분리하여 관리할 수 있다.

Namespace는 기존에 잘 알려진 가상화 기술인 Hypervisor 와는 구조적으로 다르다.

Hypervisor는 Hardware resource 를 가상화 한다. Hypervisor 위에 올라가는 Guest OS 에는 가상화 된 형태의 H/W 를 제공하게 되며, 따라서 각각의 Guest OS는 완전한 다른 환경으로 분리된다. 

하지만 Namespace의 경우에는 Hardware resource 레벨의 가상화가 아니다. 동일한 OS와 동일한 kernel 에서 작동하게 되며, 단지 각각의 고립된 사용 환경만 제공되는 것이다.

- Namespace의 종류
    - UTS Namespace : hostname을 변경하고 분할
    - IPC Namespace : 프로세스간 통신 격리
    - PID Namespace : Process ID를 분할 관리
    - NET Namespace : Network Interface, iptables 등 network 리소스와 관련된 정보를 분할
    - User Namespace : user와 group ID를 분할 격리
    - Mount Namespace :

### Container의 동작 구조

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/8223ea06-b888-47b4-b156-4b06e7091fea/_2019-10-25__10.36.19.png

---

# Hypervisor란?

> 하이퍼바이저(hypervisor)는 호스트 컴퓨터에서 다수의 운영 체제(operating system)를 동시에 실행하기 위한 논리적 플랫폼(platform)을 말한다. 가상화 머신 모니터 또는 가상화 머신 매니저(virtual machine monitor 또는 virtual machine manager, 줄여서 VMM)라고도 부른다.
> 

하이퍼 바이저(Hypervisor)는 호스트 컴퓨터 1대에서 다수의 운영체제(OS)를 동시에 실행할 수 있도록 해주는 가상 플랫폼 기술(소프트웨어)을 말한다. 하이퍼 바이저 위에 올라온 운영체제는 *게스트 OS* 혹은 *인스턴스* 라고 부른다.

하이퍼 바이저가 하는일이 정확히 무엇이며 왜 필요할까?

윈도우와 리눅스 가상머신이 동시에 돌아가고있다고 생각해보자.

각 OS는 커널이라는 녀석이 자원관리와 명령 해석을 포함한 거의 모든것을 컨트롤 하는데,

문제는 이 커널에서 사용하는 규칙이 OS마다 다르다는 것이다.

각 가상머신에서는 CPU 혹은 메모리 같은 자원을 사용하기 위해 커널에서 여러가지 명령을 날리는데, 이 규칙이 모두 다른 것이다.

하드웨어는 어느 장단에 맞추어야 할까?

이런 부분을 중재해주는 녀석이 바로 Hypervisor이다.

즉, Hypervisor는 게스트OS들에게 자원을 나눠주며 조율하고 각 게스트 OS들의 커널을 번역해서 하드웨어가 알 수 있도록 도와준다.

## Hypervisor의 타입

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/87af0857-85cb-4301-b8c2-f0ad2c54854a/Hyperviseur.png

- TYPE 1 : Native 혹은 Bare-metal

호스트 OS가 없이 하이퍼바이저가 하드웨어에 직접 설치되어 운영 체제가 프로그램을 제어하듯이 하이퍼바이저가 해당 하드웨어에서 직접 실행되며 게스트 OS를 설치할 수 있는 여러 개의 가상 머신으로 분할해준다.  게스트 OS는 하드웨어 위에서 2번째 수준으로 실행된다.

Hypervisor 자체에서 직접 하드웨어를 제어하기 때문에 Hosted 방식에 비해 오버헤드가 적다.

종류로는 Xen(반가상화), [KVM(Kernel-based Virtual Machine: 커널 기반 가상 머신)](https://www.redhat.com/ko/topics/virtualization/what-is-KVM) 등이 있다.

- TYPE 2 : Hosted

하이퍼바이저는 일반 프로그램과 같이 호스트 OS에서 실행되며 VM 내부에서 동작되는 게스트 OS는 하드웨어에서 3번째 수준으로 실행된다. 

VM의 대표적인 종류는 VMware Server, VMware Workstation, VMware Fusion, QEMU, 마이크로소프트의 버추얼 PC와 버추얼 서버, Oracle(SUN)의 버추얼박스, SWsoft의 Parallels Workstation과 Parallels Desktop이 있다.

Native 혹은 Bare-metal에 비해 오버헤드가 크지만 게스트 OS에 제약이 적고 도입이 쉽다.

<aside>
💡 추가적으로 TYPE 1의 Bare-metal 방식의 Hypervisor는 DOM0라는 것을 사용하는데 이것에 의해 전가상화, 반가상화로 구분될 수 있다.

</aside>

*TYPE 1 - Bare-metal의 가상화*

- 전가상화(Full Virtualization)
    - 각 게스트OS는 DOM0에게 명령을 전달하고, DOM0에서 이를 해석해 Hypervisor에게 알려준다.
    - 그럼 Hypervisor는 다시 하드웨어에게 해석된 최종 명령을 전달한다.
    - 따라서 모든 게스트OS의 명령에 DOM0이 개입하기 때문에 성능이 느리다.

![Supervisor가 아닌 Hyprevisor 입니다. (그림은 오타..)](https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/42a3eb95-b2dc-4bc9-8f7d-228ad4a645b6/full.png)

Supervisor가 아닌 Hyprevisor 입니다. (그림은 오타..)

- 반가상화(Para Virtualization)
    - 전가상화에서 DOM0이 수행했던 '해석'의 역할을 각 게스트OS에서 처리하는 것이다.
    - 각 게스트OS에서 해석된 명령을 DOM0을 통해서가 아닌 Hypervisor에게 바로 전달하기 때문에 전가상화에 비해 성능이 좋다.
    - 이 때 쓰는 명령을 Hyper-Call이라고 하는데, 기본적으로 모든 OS의 커널은 이러한 Hyper-Call에 대해 구현된 부분이 없어 직접 구현 해주어야 한다. 즉, 게스트OS의 커널을 직접 수정해야 한다.
    - 따라서 리눅스 등 오픈소스 OS가 아니면 반가상화를 하기 어렵다.

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/7c2f93a3-561c-40c6-b6c1-58b586771c0d/para.png

---

# [Hypervisor vs Container](https://kubernetes.io/ko/docs/concepts/overview/what-is-kubernetes/#%EC%97%AC%EC%A0%95-%EB%8F%8C%EC%95%84%EB%B3%B4%EA%B8%B0)

!https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg

**전통적인 배포 시대(Traditional Deployment):** 

초기 조직은 애플리케이션을 물리 서버에서 실행했었다. 

한 물리 서버에서 여러 애플리케이션의 리소스 한계를 정의할 방법이 없었기에, 리소스 할당의 문제가 발생했다. 

예를 들어 물리 서버 하나에서 여러 애플리케이션을 실행하면, 리소스 전부를 차지하는 애플리케이션 인스턴스가 있을 수 있고, 결과적으로는 다른 애플리케이션의 성능이 저하될 수 있었다. 

이에 대한 해결책은 서로 다른 여러 물리 서버에서 각 애플리케이션을 실행하는 것이 있다. 

그러나 이는 리소스가 충분히 활용되지 않는다는 점에서 확장 가능하지 않았으므로, 물리 서버를 많이 유지하기 위해서 조직에게 많은 비용이 들었다.

**가상화된 배포 시대(Virtualized Deployment):** 

그 해결책으로 가상화가 도입되었다. 

이는 단일 물리 서버의 CPU에서 여러 가상 시스템 (VM)을 실행할 수 있게 한다. 

가상화를 사용하면 VM간에 애플리케이션을 격리하고 애플리케이션의 정보를 다른 애플리케이션에서 자유롭게 액세스 할 수 없으므로, 일정 수준의 보안성을 제공할 수 있다.

가상화를 사용하면 물리 서버에서 리소스를 보다 효율적으로 활용할 수 있으며, 쉽게 애플리케이션을 추가하거나 업데이트할 수 있고 하드웨어 비용을 절감할 수 있어 더 나은 확장성을 제공한다.

각 VM은 가상화된 하드웨어 상에서 자체 운영체제를 포함한 모든 구성 요소를 실행하는 전체 시스템이다.

**컨테이너 개발 시대(Container Deployment):** 

컨테이너는 VM과 유사하지만 격리 속성을 완화하여 애플리케이션 간에 운영체제(OS)를 공유한다. 

**그러므로 컨테이너는 가볍다고 여겨진다.** 

VM과 마찬가지로 컨테이너에는 자체 파일 시스템, CPU, 메모리, 프로세스 공간 등이 있다. 

**기본 인프라와의 종속성을 끊었기 때문에, 클라우드나 OS 배포본에 모두 이식할 수 있다.**

컨테이너의 **장점**의 일부는 다음과 같다.

- 기민한 애플리케이션 생성과 배포: VM 이미지를 사용하는 것에 비해 컨테이너 이미지 생성이 보다 쉽고 효율적임.
- 지속적인 개발, 통합 및 배포: 안정적이고 주기적으로 컨테이너 이미지를 빌드해서 배포할 수 있고 (이미지의 불변성 덕에) 빠르고 쉽게 롤백할 수 있다.
- 개발과 운영의 관심사 분리: 배포 시점이 아닌 빌드/릴리스 시점에 애플리케이션 컨테이너 이미지를 만들기 때문에, 애플리케이션이 인프라스트럭처에서 디커플된다.
- 가시성은 OS 수준의 정보와 메트릭에 머무르지 않고, 애플리케이션의 헬스와 그 밖의 시그널을 볼 수 있다.
- 개발, 테스팅 및 운영 환경에 걸친 일관성: 랩탑에서도 클라우드에서와 동일하게 구동된다.
- 클라우드 및 OS 배포판 간 이식성: Ubuntu, RHEL, CoreOS, on-prem, Google Kubernetes Engine 및 다른 어디에서든 구동된다.
- 애플리케이션 중심 관리: 가상 하드웨어의 OS에서 애플리케이션을 구동하는 수준에서 OS의 논리적인 자원을 사용하여 애플리케이션을 구동하는 수준으로 추상화 수준이 높아진다.
- 느슨하게 커플되고, 분산되고, 유연하며, 자유로운 마이크로서비스: 애플리케이션은 단일 목적의 머신에서 모놀리식 스택으로 구동되지 않고 보다 작고 독립적인 단위로 쪼개져서 동적으로 배포되고 관리될 수 있다.
- 자원 격리: 애플리케이션 성능을 예측할 수 있다.
- 자원 사용량: 고효율 고집적.

실제 하드웨어인 것처럼 에뮬레이션(emulation)을 하는 VM과 달리 container는 호스트 PC의 자원을 격리(isolation)된 상태로 그대로 활용하기 때문에 VM에 비해 성능 저하가 눈에 띄게 적다.

따라서 Hypervisor를 사용하는 것 보다 Container를 사용하는 것이 더욱 효율적이다.

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/3d9bc129-2572-4305-83a8-ebc89185fdbb/vm-vs-docker.png

---

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/76a32a97-791a-47bc-bc59-f647eeba8f35/docker-works.png

# Docker란?

> Container 기반의 오픈소스 가상화 플랫폼
> 

Docker란 Go언어로 만든 오픈소스 프로젝트이며, 컨테이너 기술을 쉽고 간단하게 사용할 수 있도록 다양한 커맨드라인이나 API를 제공해주는 오픈소스이다. Docker를 구성하는 요소들로 Docker Image와 Layer, Dockerfile 그리고 Docker의 Command line Interface 및 API, Docker Hub(Docker의 Git Hub)가 있다.

# Docker Engine

![Docker Engine](https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/5887568f-44dc-44f8-8b58-a95834791875/Untitled.png)

Docker Engine

*[Docker Engine](https://github.com/docker/engine/tree/ac7306503d237d548e376a89ab0b899ea1a245b0)* 은 다음과 같은 주요 구성 요소가 포함 된 클라이언트-서버 응용 프로그램이다.

- 데몬 프로세스 ( `dockerd`명령) 라고하는 장기 실행 프로그램 유형의 서버이다.
- 프로그램이 [데몬](https://github.com/docker/engine/tree/ac7306503d237d548e376a89ab0b899ea1a245b0/daemon)과 통신하고 수행 할 작업을 지시하는 데 사용할 수있는 인터페이스를 지정하는 [REST API](https://github.com/docker/engine/tree/ac7306503d237d548e376a89ab0b899ea1a245b0/api).
- [CLI (명령 줄 인터페이스) 클라이언트](https://github.com/docker/cli/tree/3d35fd40d2c522a8670f0eb3f0975bcb5f8ee4a7) ( `docker`명령)

CLI는 Docker REST API를 사용하여 스크립팅 또는 직접 CLI 명령을 통해 Docker 데몬을 제어하거나 상호 작용한다. 다른 많은 Docker 응용 프로그램은 기본 API 및 CLI를 사용한다.

데몬은 이미지, 컨테이너, 네트워크 및 볼륨과 같은 Docker *객체를* 만들고 관리한다.

docker cli를 통해 REST API에 요청 전달 → API가 요청을 받아서 docker daemon을 통해 cgroup, namespace 등 기능을 사용하여 커널 레벨에서 호스트의 자원을 격리

# Docker Architecture

![Docker Architecture Diagram](https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/148ea428-24eb-431a-802e-d273a5fa657a/Untitled.png)

Docker Architecture Diagram

Docker는 클라이언트-서버 아키텍처를 사용한다. 

Docker 클라이언트는 Docker 데몬과 통신한다.

Docker 데몬은 Docker 컨테이너를 빌드, 실행 및 배포하는 작업을 많이 수행한다.

Docker 클라이언트와 데몬은 동일한 시스템에서 실행되거나 Docker 클라이언트를 원격 Docker 데몬에 연결할 수 있다.

Docker 클라이언트와 데몬은 REST 소켓, UNIX 소켓 또는 네트워크 인터페이스를 통해 통신한다.

## Docker Daemon

Docker 데몬([dockerd](https://docs.docker.com/engine/reference/commandline/dockerd/))은 Docker API 요청을 수신하고 이미지, 컨테이너, 네트워크 및 볼륨과 같은 Docker 객체를 관리한다.

데몬은 다른 데몬과 통신하여 Docker 서비스를 관리 할 수도 있다.

## Docker Client

Docker 클라이언트(docker)는 많은 Docker 사용자가 Docker와 상호 작용하는 기본 방법이다.

`docker run`과 같은 명령을 사용 하면 클라이언트는 이 명령을 `dockerd`에 보내어 명령을 수행한다.

`docker`명령은 **Docker API**를 사용한다.

Docker 클라이언트는 둘 이상의 데몬과 통신 할 수 있다.

## Docker Registry

Docker *레지스트리* 는 Docker 이미지를 저장한다.

Docker Hub는 누구나 사용할 수있는 공개 레지스트리이며 Docker는 기본적으로 Docker Hub에서 이미지를 찾도록 구성되어 있다. 

자신의 개인 레지스트리를 실행할 수도 있다. 

Docker Datacenter(DDC)를 사용하는 경우 Docker Trusted Registry(DTR)가 포함된다.

`docker pull`또는 `docker run`명령 을 사용하면 필요한 이미지를 구성된 레지스트리에서 가져온다. 

`docker push`명령 을 사용하면 이미지가 구성된 레지스트리로 푸시된다.

## Docker Object

Docker를 사용하면 이미지, 컨테이너, 네트워크, 볼륨, 플러그인 및 기타 객체를 생성 및 사용하게 된다. 

이 섹션은 이러한 개체 중 일부에 대한 간략한 개요이다.

### Image

이미지는 도커 컨테이너를 생성하기 위한 읽기 전용 템플릿이다.

종종 이미지는 다른 이미지를 기반으로 하며 추가적으로 사용자 정의가 있을 수 있다.

예를 들어, Ubuntu 이미지를 기반으로 하는 이미지를 만들 수 있지만 Apache 웹 서버와 응용 프로그램 및 응용 프로그램을 실행하는 데 필요한 구성을 할 수 있다.

자신의 이미지를 만들거나 다른 사람들이 만들어 레지스트리에 게시한 이미지만 사용할 수 있다.

자신의 이미지를 만들려면 이미지를 작성하고 실행하는 데 필요한 단계를 정의하기 위해 Dockerfile을 생성하여 간단한 구문을 작성 해야 한다.

Dockerfile의 각 명령어는 이미지에 레이어를 만든다.

Dockerfile을 변경하고 이미지를 다시 만들면 변경된 레이어만 다시 작성된다.

이것은 다른 가상화 기술과 비교할 때 이미지를 매우 작고 빠르게 만드는 솔루션 중 하나이다.

### Container

컨테이너는 실행 가능한 이미지 인스턴스이다.

Docker API 또는 CLI를 사용하여 컨테이너를 생성, 시작, 중지, 이동 또는 삭제를 할 수 있다.

또한 컨테이너를 하나 이상의 네트워크에 연결하거나, 스토리지를 연결하거나, 현재 상태에 따라 새 이미지를 만들 수 있다.

기본적으로 컨테이너는 다른 컨테이너 및 해당 호스트 시스템과 비교적 잘 격리 되어있다.

컨테이너의 네트워크, 스토리지 또는 기타 기본 하위 시스템이 다른 컨테이너 또는 호스트 시스템과 분리되는 방식을 제어할 수 있다.

컨테이너는 이미지 뿐만 아니라 이미지를 만들거나 시작할 때 제공하는 구성 옵션으로 정의된다.

컨테이너를 제거하면 영구 저장소에 저장되지 않은 상태 변경 내용은 사라진다.

---

# 기본 기술

- Namespaces
- Control groups(cgroups)
- Union file systems
- Container format

## Namespaces

도커는 네임스페이스라는 기술을 사용하여 컨테이너라는 격리된 작업 공간을 제공한다.

컨테이너를 실행할 때 도커는 해당 컨테이너에 대한 네임스페이스 집합을 만든다.

이러한 네임스페이스는 격리 계층을 제공하는데, 컨테이너의 각 측면은 별도의 네임스페이스에서 실행되며 해당 네임스페이스로 액세스가 제한된다.

도커 엔진은 Linux에서 다음과 같은 네임스페이스를 사용한다.

- PID 네임스페이스 : 프로세스 격리(PID: Process ID)
- NET 네임스페이스 : 네트워크 인터페이스 격리(NET: networking)
- IPC 네임스페이스 : IPC 자원에 대한 액세스 관리(IPC: InterProcess Communication : 프로세스 간 통신)
- MNT 네임스페이스 : 파일 시스템 마운트 지점 관리(MNT: Mount)
- UTS 네임스페이스 : 커널 및 버젼 식별자 격리(UTS: Unix Timesharing System : 유닉스 시분할 시스템)

## Control Groups

Linux의 도커 엔진은 컨트롤 그룹(cgroups)이라는 기술에 의존한다.

cgroup은 응용 프로그램을 특정 리소스 집합으로 제한한다.

컨트롤 그룹을 통해 도커 엔진은 사용 가능한 하드웨어 자원을 컨테이너와 공유하고 선택적으로 한계 및 제한 조건을 시행할 수 있다.

예를 들어 특정 컨테이너에 사용 가능한 메모리를 제한할 수 있다.

## Union file systems

UnionFS(Union file system)는 계층을 만들어서 매우 가볍고 빠르게 작동하는 파일 시스템이다.

도커 엔진은 UnionFS를 사용하여 컨테이너의 빌딩 블록을 제공한다.

또한 도커 엔진은 AUFS, vfs 및 DeviceMapper를 포함한 여러 UnionFS 변형을 사용할 수 있다.

## Container format

도커 엔진은 네임스페이스, 컨트롤 그룹, UnionFS를 컨테이너 형식(Container format)이라는 형식으로 결합한다.

기본 컨테이너 형식은 `libcontainer` 이다.

앞으로 도커는 `BSD Jails` 또는 `Solaris Zones`와 같은 기술과 통합하여 다른 컨테이너 형식을 지원하려 하고 있다.

---

## Docker Image 세부 정보

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/9806cb66-579e-49ee-910b-eb6fc39c06f6/image-url.png

이미지는 url 방식으로 관리하며 태그를 붙일 수 있다. 

ubuntu 14.04 이미지는 [docker.io/library/ubuntu:14.04](http://docker.io/library/ubuntu:14.04) 또는 [docker.io/library/ubuntu:trusty](http://docker.io/library/ubuntu:trusty) 이고 [docker.io/library는](http://docker.io/library%EB%8A%94) 생략 가능하여 ubuntu:14.04 로 사용할 수 있다. 이러한 방식은 이해하기 쉽고 편리하게 사용할 수 있으며 태그 기능을 잘 이용하면 테스트나 롤백도 쉽게 할 수 있다.

```docker
# vertx/vertx3 debian version
FROM subicura/vertx3:3.3.1
MAINTAINER chungsub.kim@purpleworks.co.kr

ADD build/distributions/app-3.3.1.tar /
ADD config.template.json /app-3.3.1/bin/config.json
ADD docker/script/start.sh /usr/local/bin/
RUN ln -s /usr/local/bin/start.sh /start.sh

EXPOSE 8080
EXPOSE 7000

CMD ["start.sh"]
```

도커는 [이미지를 만들기 위해 `Dockerfile`이라는 파일에 자체 DSLDomain-specific language언어를 이용하여 이미지 생성 과정을 적는다](https://docs.docker.com/engine/reference/builder/). 위 샘플을 보면 그렇게 복잡하지 않다는 걸 알 수 있다.

이것은 굉장히 간단하지만 유용한 아이디어인데, 서버에 어떤 프로그램을 설치하려고 이것저것 의존성 패키지를 설치하고 설정 파일을 만들었던 경험이 있다면 더 이상 그 과정을 블로깅 하거나 메모장에 적지 않고 `Dockerfile`로 관리하면 된다. 이 파일은 소스와 함께 버전 관리 되고 원한다면 누구나 이미지 생성 과정을 보고 수정할 수 있다.

## Layer 저장 방식

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/1da0545a-94e5-48e0-b93c-c2f00b5fe2ff/image-layer.png

도커 이미지는 컨테이너를 실행하기 위한 모든 정보를 가지고 있기 때문에 보통 용량이 수백MB에 이른다. 

처음 이미지를 다운받을 땐 크게 부담이 안되지만 기존 이미지에 파일 하나 추가했다고 수백MB를 다시 다운받는다면 매우 비효율적일 수 밖에 없다.

도커는 이런 문제를 해결하기 위해 **레이어(layer)**라는 개념을 사용하고 유니온 파일 시스템을 이용하여 여러개의 레이어를 하나의 파일시스템으로 사용할 수 있게 해준다. 

이미지는 여러개의 읽기 전용 read only 레이어로 구성되고 파일이 추가되거나 수정되면 새로운 레이어가 생성된다.

ubuntu 이미지가 `A + B + C`의 집합이라면, 

ubuntu 이미지를 베이스로 만든 nginx 이미지는 `A + B + C + nginx`가 된다. 

webapp 이미지를 nginx 이미지 기반으로 만들었다면 

예상대로 `A + B + C + nginx + source` 레이어로 구성된다. 

webapp 소스를 수정하면 

A, B, C, nginx 레이어를 제외한 새로운 source(v2) 레이어만 다운받으면 되기 때문에 굉장히 효율적으로 이미지를 관리할 수 있다.

컨테이너를 생성할 때도 레이어 방식을 사용하는데 기존의 이미지 레이어 위에 읽기/쓰기(read-write) 레이어를 추가한다. 이미지 레이어를 그대로 사용하면서 컨테이너가 실행 중에 생성하는 파일이나 변경된 내용은 읽기/쓰기 레이어에 저장되므로 여러 개의 컨테이너를 생성해도 최소한의 용량만 사용한다.

가상화의 특성상 이미지 용량이 크고 여러 대의 서버에 배포하는걸 감안하면 단순하지만 엄청나게 영리한 설계이다.

## [Docker Hub](https://hub.docker.com/)

Docker는 Docker Hub를 통해 공개 이미지를 무료로 관리해준다.

[간단한 도커의 커맨드라인 예제](https://github.com/BangShinChul/Docker-study)

---

# Docker Networking

Linux에서 도커는 iptables 규칙을 조작하여 네트워크 격리를 제공한다.

이것은 구현 세부 사항이며 도커가 iptables 정책에 삽입하는 규칙을 수정해서는 안된다.

아래는 도커 네트워크의 종류이다.

- [Bridge 네트워크](https://docs.docker.com/v18.09/network/bridge/)
- [Host 네트워크](https://docs.docker.com/v18.09/network/host/)
- [Overlay 네트워크](https://docs.docker.com/v18.09/network/overlay/)
- [macvlan 네트워크](https://docs.docker.com/v18.09/network/macvlan/)

기본적으로 도커의 네트워크는 bridge 네트워크를 사용한다.

도커의 네트워크를 조회하는 명령어 :

`sudo docker network ls`

도커의 네트워크 중 하나를 상세 조회하는 명령어 : 

예시) bridge network

`sudo docker network inspect bridge`

각 컨테이너는 생성될 때 도커 데몬으로부터 네트워크에 대한 정보를 전달받는다.

도커 데몬이 컨테이너에게 DHCP 서버의 역할을 하는 것이다.

아래와 같이 `docker network inspect` 명령어를 통해 네트워크의 정보를 조회해보면 컨테이너의 MAC 주소와 IPv4 주소 및 서브넷 마스크 정보까지 알 수 있다.

```bash
$ sudo docker network inspect bridge
[
    {
        "Name": "bridge",
        "Id": "926107b934decf7b5129ce4d68f73b29e3cce19713690084bf1947be6d683422",
        "Created": "2019-11-04T02:40:37.907885457Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "5c5214370dfb3e08ec13dc25540704bfcaf0d10472a999da35e149b7e9d366b4": {
                "Name": "focused_liskov",
                "EndpointID": "3bafab184adbdd2dd320722352e0686881c5ba12a54c39f0bbae09e129a9d57c",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]
```

---

# Docker Storage

기본적으로 컨테이너 내부에 생성된 모든 파일은 쓰기 가능한 컨테이너 계층에 저장된다.

이것은 다음을 의미한다.

- 해당 컨테이너가 더이상 존재하지 않으면 데이터가 지속되지 않으며 다른 프로세스에 필요한 경우 컨테이너에서 데이터를 가져오는 것이 어려울 수 있다.
- 컨테이너의 쓰기 가능한 계층은 컨테이너가 실행 중인 호스트 시스템에 단단히 연결된다. 데이터를 다른 곳으로 쉽게 이동할 수 없다.
- 컨테이너의 쓰기 가능한 레이어에 쓰려면 파일 시스템을 관리하기 위한 스토리지 드라이버가 필요하다. 스토리지 드라이버는 Linux 커널을 사용하여 통합 파일 시스템을 제공한다. 이 추가적인 추상화는 호스트 파일 시스템에 직접 쓰는 데이터 볼륨을 사용하는 것과 비교하여 성능을 저하시킨다.

도커에는 컨테이너가 호스트 시스템에 파일을 저장하는 두 가지 옵션이 있으므로 컨테이너가 중지된 후에도 파일을 유지시킬 수 있다. (Volume 및 Bind Mount)

Linux에서 도커를 실행중인 경우 tmpfs 마운트를 사용할 수도 있다.

!https://prod-files-secure.s3.us-west-2.amazonaws.com/a156cc82-d201-4160-ac40-16f8ab23f7dc/44c28d4d-0e78-43aa-81f7-913541445081/Untitled.png

**[볼륨(Volume)](https://docs.docker.com/v18.09/storage/volumes/)**은 도커가 관리하는 호스트 파일 시스템의 일부(Linux의 경우 /var/lib/docker/volumes/)에 저장된다.

도커가 아닌 프로세스는 파일 시스템의 이 부분을 수정하면 안된다.

볼륨은 도커에서 데이터를 유지하는 가장 좋은 방법이다.

**[바인드 마운트(Bind Mount)](https://docs.docker.com/v18.09/storage/bind-mounts/)**는 호스트 시스템의 어느 곳에나 저장될 수 있다.

볼륨과는 다르게 호스트 시스템의 어느 곳에나 파일이 존재할 수 있기 때문에 도커 이외의 프로세스도 파일을 수정할 수 있다.

**[tmpfs 마운트](https://docs.docker.com/v18.09/storage/tmpfs/)**는 호스트 시스템의 메모리에만 저장되며 호스트 시스템의 파일 시스템에는 기록되지 않는다.

이 외에 [컨테이너 내에 데이터를 저장하는 방법](https://docs.docker.com/v18.09/storage/storagedriver/)도 있다.
